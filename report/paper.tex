\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, decorations.pathreplacing}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Comparing CNN Architectures for Print-to-Camera Image Restoration}

\author{\IEEEauthorblockN{Daniel L. Lau}
\IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering} \\
\textit{University of Kentucky}\\
Lexington, KY, USA \\
dllau@uky.edu}}

\maketitle

\begin{abstract}
Machine vision systems used in print quality inspection face a fundamental challenge: camera-captured images of printed materials exhibit systematic degradations including color shifts, reduced contrast, and scanning artifacts that differ from the original digital source images. This paper compares two convolutional neural network architectures for learning the inverse mapping from captured print images to pristine originals: (1) a conditional GAN using the pix2pix framework with U-Net generator, perceptual loss, and adversarial training, and (2) NAFNet, a recent activation-free architecture designed for image restoration. Experimental results on a dataset of 16,193 paired print-capture images demonstrate that pix2pix achieves 26.65~dB PSNR and 0.75 SSIM, outperforming NAFNet (24.54~dB PSNR, 0.74 SSIM) and traditional baselines by significant margins. Both deep learning approaches substantially exceed traditional color transfer methods, with pix2pix providing an 8.6~dB improvement over the best conventional baseline. Our findings suggest that adversarial training provides meaningful benefits for this domain adaptation task.
\end{abstract}

\begin{IEEEkeywords}
image-to-image translation, generative adversarial networks, NAFNet, print quality inspection, machine vision
\end{IEEEkeywords}

\section{Introduction}

Industrial print quality inspection systems rely on machine vision cameras to capture images of printed materials for automated defect detection and quality assessment. A critical challenge in these systems is the systematic degradation introduced during the print-capture pipeline: even when printed images are visually near-perfect renditions of their digital sources, the captured images exhibit noticeable differences including color shifts, contrast reduction, geometric distortions, and sensor-specific artifacts.

These degradations arise from multiple sources in the imaging chain. The printing process itself introduces halftoning patterns, ink absorption variations, and substrate interactions. The capture process adds camera sensor noise, optical aberrations, and illumination non-uniformities. While individual degradations may be subtle, their cumulative effect creates a significant domain gap between digital originals and their printed-then-captured counterparts.

Traditional approaches to this problem include color calibration using reference targets, image registration with geometric correction, and histogram-based color mapping. However, these methods address individual degradations in isolation and struggle to model the complex, nonlinear interactions between printing and capture artifacts. Furthermore, they require manual tuning and domain expertise to achieve acceptable results.

We propose a data-driven approach using deep convolutional neural networks to learn the complete inverse transformation from captured images to original digital sources (Fig.~\ref{fig:pipeline}). By training on paired examples of original images and their printed-captured counterparts, these models learn to jointly correct all degradations without explicit modeling of individual artifact sources.

This paper compares two representative CNN architectures for this task: (1) pix2pix~\cite{isola2017image}, a conditional GAN combining a U-Net generator with adversarial and perceptual losses, and (2) NAFNet~\cite{chen2022nafnet}, a recent activation-free architecture that achieved state-of-the-art results on standard image restoration benchmarks. These architectures represent different design philosophies: pix2pix leverages adversarial training to produce perceptually realistic outputs, while NAFNet uses a simplified feedforward design optimized for reconstruction accuracy.

Our key contributions are:
\begin{itemize}
\item A comparative evaluation of GAN-based and non-GAN CNN architectures for print-to-camera image restoration
\item Demonstration that adversarial training provides measurable benefits for this domain adaptation task
\item A bidirectional framework supporting both artifact simulation (forward) and quality restoration (reverse)
\end{itemize}

\section{Related Work}

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    node distance=0.3cm,
    box/.style={rectangle, draw, minimum width=1.6cm, minimum height=1cm, align=center, font=\scriptsize},
    arrow/.style={-{Stealth[length=2mm]}, thick},
    label/.style={font=\tiny, align=center}
]
\node[box] (source) {\includegraphics[width=1.1cm,height=0.8cm]{fig_original.png}};
\node[label, below=0.02cm of source] {Digital Source};

\node[box, right=0.5cm of source] (printer) {
    Inkjet\\Printhead\\[-2pt]
    \tikz[baseline=-0.5ex]{\draw[thick] (0,0) -- (0.7,0); \foreach \x in {0,0.07,...,0.7} \draw (\x,0) -- (\x,-0.06);}\\[-2pt]
    {\tiny Paper Roll}
};
\node[label, below=0.02cm of printer] {\parbox{1.2cm}{\centering\tiny Continuous Roll\\Label Printer}};

\node[box, right=0.5cm of printer] (camera) {
    Line-Scan\\Camera\\[1pt]
    \tikz[baseline=-0.5ex]{
        \draw[thick] (0,0.15) -- (0,-0.15);
        \draw[thick] (-0.15,0.12) -- (0,0) -- (-0.15,-0.12);
    }
};
\node[label, below=0.02cm of camera] {Vision System};

\node[box, right=0.5cm of camera] (captured) {\includegraphics[width=1.1cm,height=0.8cm]{fig_scanned.png}};
\node[label, below=0.02cm of captured] {Captured Image};

\draw[arrow] (source) -- (printer);
\draw[arrow] (printer) -- (camera);
\draw[arrow] (camera) -- (captured);
\end{tikzpicture}%
}
\caption{Print quality inspection pipeline. Digital images are printed on a continuous roll label printer, then captured by a line-scan machine vision camera.}
\label{fig:pipeline}
\end{figure}

\subsection{Image-to-Image Translation}

The pix2pix framework introduced by Isola et al.~\cite{isola2017image} established conditional GANs as a general-purpose solution for paired image-to-image translation. The architecture combines a U-Net generator with skip connections and a PatchGAN discriminator that classifies local image regions. This approach has been successfully applied to tasks including semantic segmentation, colorization, and style transfer.

CycleGAN~\cite{zhu2017unpaired} extended this paradigm to unpaired training using cycle-consistency loss, enabling translation between domains without explicit correspondences. While powerful, unpaired methods may introduce semantic changes inappropriate for quality-critical applications where pixel-accurate restoration is required.

Recent advances include attention mechanisms~\cite{chen2018attention}, multi-scale discriminators~\cite{wang2018high}, and diffusion-based approaches~\cite{saharia2022palette}. However, the original pix2pix architecture remains effective for applications with well-aligned paired data.

\subsection{Image Restoration Networks}

Beyond GAN-based approaches, recent work has explored simplified architectures for image restoration. NAFNet (Nonlinear Activation Free Network)~\cite{chen2022nafnet} demonstrated that state-of-the-art results on denoising and deblurring benchmarks can be achieved without traditional nonlinear activations like ReLU. Instead, NAFNet uses element-wise multiplication (gating) as its only nonlinearity, combined with simplified channel attention. This design achieves competitive performance with reduced computational complexity. We include NAFNet in our comparison to evaluate whether these architectural simplifications transfer to the print-capture restoration domain.

\subsection{Print Quality and Document Analysis}

Document image enhancement has been extensively studied for applications including OCR preprocessing and historical document restoration. Hradis et al.~\cite{hradis2015convolutional} applied convolutional networks to document deblurring. More recent work has explored end-to-end learning for document binarization~\cite{tensmeyer2017document} and degradation removal~\cite{souibgui2020gan}.

Color management in print workflows traditionally relies on ICC profiles and colorimetric calibration~\cite{sharma2018understanding}. While effective for average color accuracy, these approaches do not address spatially-varying artifacts or texture degradations introduced during capture.

\subsection{Perceptual Loss Functions}

Johnson et al.~\cite{johnson2016perceptual} demonstrated that optimizing in the feature space of pretrained networks produces more visually pleasing results than pixel-wise losses alone. Perceptual loss computed using VGG features has become standard in image restoration tasks, enabling networks to match high-level image structure while allowing flexibility in low-level details.

\section{Methodology}

\subsection{Problem Formulation}

Let $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$ denote an original digital image and $\mathbf{y} \in \mathbb{R}^{H \times W \times 3}$ its corresponding printed-and-captured version. We seek to learn a mapping $G: \mathbf{y} \rightarrow \mathbf{x}$ that restores the original image quality from the degraded capture.

For bidirectional modeling, we also train a forward model $G_f: \mathbf{x} \rightarrow \mathbf{y}$ that predicts how images will appear after printing and capture. Both models share identical architectures but are trained on data with swapped input-target roles.

\subsection{Network Architectures}

We compare two CNN architectures that share a common encoder-decoder structure with skip connections but differ significantly in their design philosophy and training methodology.

\begin{figure}[t]
\centering
\resizebox{0.95\columnwidth}{!}{%
\begin{tikzpicture}[
    block/.style={rectangle, draw, minimum width=0.4cm, fill=blue!20},
    arrow/.style={-{Stealth[length=2mm]}, thick},
    skip/.style={-{Stealth[length=2mm]}, thick, dashed, gray}
]
% Encoder (left side) - blocks get shorter as we go down
\node[block, minimum height=2.0cm] (e1) at (0,0) {};
\node[block, minimum height=1.6cm, fill=blue!30] (e2) at (0.7,-0.2) {};
\node[block, minimum height=1.2cm, fill=blue!40] (e3) at (1.4,-0.4) {};
\node[block, minimum height=0.8cm, fill=blue!50] (e4) at (2.1,-0.6) {};

% Bottleneck
\node[block, minimum height=0.5cm, fill=red!40, minimum width=0.5cm] (b) at (2.9,-0.75) {};

% Decoder (right side) - blocks get taller as we go up
\node[block, minimum height=0.8cm, fill=green!50] (d4) at (3.7,-0.6) {};
\node[block, minimum height=1.2cm, fill=green!40] (d3) at (4.4,-0.4) {};
\node[block, minimum height=1.6cm, fill=green!30] (d2) at (5.1,-0.2) {};
\node[block, minimum height=2.0cm, fill=green!20] (d1) at (5.8,0) {};

% Arrows down (encoder)
\draw[arrow] (e1.south) -- ++(0,-0.15) -| (e2.north);
\draw[arrow] (e2.south) -- ++(0,-0.15) -| (e3.north);
\draw[arrow] (e3.south) -- ++(0,-0.15) -| (e4.north);
\draw[arrow] (e4.east) -- (b.west);

% Arrows up (decoder)
\draw[arrow] (b.east) -- (d4.west);
\draw[arrow] (d4.north) -- ++(0,0.15) -| (d3.south);
\draw[arrow] (d3.north) -- ++(0,0.15) -| (d2.south);
\draw[arrow] (d2.north) -- ++(0,0.15) -| (d1.south);

% Skip connections
\draw[skip] (e4.east) -- ++(0.2,0) |- ([yshift=0.15cm]d4.west);
\draw[skip] (e3.east) -- ++(0.2,0) |- ([yshift=0.2cm]d3.west);
\draw[skip] (e2.east) -- ++(0.2,0) |- ([yshift=0.25cm]d2.west);
\draw[skip] (e1.east) -- ++(0.2,0) |- ([yshift=0.3cm]d1.west);

% Labels
\node[above, font=\tiny] at (e1.north) {64};
\node[above, font=\tiny] at (e2.north) {128};
\node[above, font=\tiny] at (e3.north) {256};
\node[above, font=\tiny] at (e4.north) {512};
\node[below, font=\tiny] at (b.south) {1024};
\node[above, font=\tiny] at (d4.north) {512};
\node[above, font=\tiny] at (d3.north) {256};
\node[above, font=\tiny] at (d2.north) {128};
\node[above, font=\tiny] at (d1.north) {64};

% Input/Output labels
\node[left, font=\tiny] at (e1.west) {Input};
\node[right, font=\tiny] at (d1.east) {Output};

% Section labels
\node[below, font=\scriptsize] at (1.05,-1.3) {Encoder};
\node[below, font=\scriptsize] at (2.9,-1.3) {Bottleneck};
\node[below, font=\scriptsize] at (4.75,-1.3) {Decoder};

\end{tikzpicture}%
}
\caption{Encoder-decoder architecture shared by both models. Blue: encoder (downsampling). Red: bottleneck. Green: decoder (upsampling). Dashed lines: skip connections. Pix2Pix uses channel concatenation for skips; NAFNet uses addition.}
\label{fig:unet}
\end{figure}

\subsubsection{Pix2Pix: U-Net Generator with GAN}

Our generator follows the U-Net architecture with symmetric encoder-decoder structure and skip connections (Fig.~\ref{fig:unet}). The encoder consists of four downsampling blocks that progressively extract features at resolutions 512, 256, 128, and 64 pixels. Each block applies two 3$\times$3 convolutions with batch normalization and LeakyReLU activation (slope 0.2), followed by 2$\times$2 max pooling.

The bottleneck operates at 32$\times$32 resolution with 1024 channels. The decoder mirrors the encoder with transposed convolutions for upsampling. Skip connections concatenate encoder features with decoder activations at each resolution level, preserving spatial details that would otherwise be lost through the bottleneck.

The final layer is a 1$\times$1 convolution producing 3-channel RGB output. The complete generator contains approximately 31 million parameters.

\subsubsection{PatchGAN Discriminator}

The discriminator follows the PatchGAN design that classifies 70$\times$70 overlapping patches rather than the entire image. This approach provides dense gradient signal and captures high-frequency structure effectively.

The discriminator receives concatenated input and target images (6 channels) and applies four convolutional blocks with stride 2, expanding channels from 64 to 512. Batch normalization is omitted in the first layer. The final layer outputs a spatial map of patch classifications.

\subsubsection{NAFNet: Activation-Free Restoration Network}

NAFNet~\cite{chen2022nafnet} takes a different approach, eliminating traditional nonlinear activations entirely. The architecture uses a similar encoder-decoder structure with four resolution levels, but replaces ReLU/LeakyReLU with SimpleGate---a parameter-free operation that splits channels in half and multiplies them element-wise. This gating mechanism provides nonlinearity without explicit activation functions.

Each NAFBlock consists of: (1) layer normalization, (2) depthwise convolution for spatial mixing, (3) SimpleGate activation, (4) simplified channel attention (global average pooling followed by a linear projection), and (5) a feedforward network with another SimpleGate. Learnable scaling parameters ($\beta$) modulate residual connections.

Unlike pix2pix, NAFNet uses additive skip connections rather than concatenation, and is trained with L1 loss only---no adversarial or perceptual losses. The model contains approximately 17 million parameters (versus 31M for pix2pix). This simplified design achieved state-of-the-art results on SIDD denoising and GoPro deblurring benchmarks.

\subsection{Loss Functions}

The total generator loss combines three terms:

\begin{equation}
\mathcal{L}_G = \lambda_1 \mathcal{L}_{L1} + \lambda_p \mathcal{L}_{perceptual} + \lambda_{adv} \mathcal{L}_{adv}
\end{equation}

\subsubsection{L1 Reconstruction Loss}

The L1 loss encourages pixel-wise accuracy:
\begin{equation}
\mathcal{L}_{L1} = \mathbb{E}[\|\mathbf{x} - G(\mathbf{y})\|_1]
\end{equation}

We weight this term heavily ($\lambda_1 = 100$) to ensure faithful reconstruction.

\subsubsection{Perceptual Loss}

Perceptual loss measures similarity in VGG-19 feature space:
\begin{equation}
\mathcal{L}_{perceptual} = \sum_{l} \|\phi_l(\mathbf{x}) - \phi_l(G(\mathbf{y}))\|_1
\end{equation}

where $\phi_l$ extracts features from VGG layers relu1\_2, relu2\_2, and relu3\_4. We use $\lambda_p = 10$.

\subsubsection{Adversarial Loss}

The adversarial loss follows the standard GAN formulation:
\begin{equation}
\mathcal{L}_{adv} = \mathbb{E}[\log(1 - D(\mathbf{y}, G(\mathbf{y})))]
\end{equation}

with $\lambda_{adv} = 1$. The discriminator is trained to maximize classification accuracy on real versus generated pairs.

\subsection{Training Procedure}

Training alternates between discriminator and generator updates. For each iteration, we first update the discriminator on a batch of real pairs (label 1) and generated pairs (label 0). The generator is then updated to minimize the combined loss while keeping discriminator weights frozen.

We use AdamW optimization with learning rate $2 \times 10^{-4}$ and momentum parameters $\beta_1 = 0.5$, $\beta_2 = 0.999$. Cosine annealing reduces the learning rate to 1\% of initial value over training. Mixed-precision training (FP16) accelerates computation and reduces memory usage.

\section{Experiments}

\subsection{Dataset}

Our dataset consists of paired images where each original digital image has a corresponding version that was printed and recaptured using an industrial machine vision camera. The source images are drawn from the Kaggle Dogs vs.\ Cats dataset~\cite{kaggle2013cats}, a widely-used benchmark in CNN research containing natural images of cats and dogs. These images provide diverse content with varying colors, textures, and lighting conditions suitable for evaluating image restoration quality. Images are stored as TIFF files to preserve quality, with pairing established through numeric identifiers in filenames.

The complete dataset contains 16,193 paired images. All images are resized and center-cropped to 512$\times$512 pixels. Pixel values are normalized to $[-1, 1]$. We use a 90/10 train/validation split with fixed random seed for reproducibility, yielding 14,574 training images and 1,619 validation images.

\subsection{Implementation Details}

Training was performed on an NVIDIA RTX 4070 Ti GPU with 12GB memory. Batch size of 4 images fits comfortably in memory with mixed-precision training. Complete training of 10,000 steps requires approximately 35 minutes.

Checkpoints are saved every 2,000 steps, enabling selection of optimal model based on validation performance. Visual results on held-out samples are generated at each checkpoint for qualitative assessment.

\subsection{Results}

\begin{table}[t]
\centering
\caption{Training Configuration}
\label{tab:config}
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Image Resolution & 512 $\times$ 512 \\
Batch Size & 4 \\
Training Steps & 10,000 \\
Learning Rate & $2 \times 10^{-4}$ \\
L1 Weight ($\lambda_1$) & 100 \\
Perceptual Weight ($\lambda_p$) & 10 \\
Adversarial Weight ($\lambda_{adv}$) & 1 \\
Optimizer & AdamW \\
Precision & FP16 (mixed) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Quantitative Analysis}

We evaluate our model on the validation set (1,619 images) using three standard image quality metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM)~\cite{wang2004ssim}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018lpips}. We compare against four baseline methods: Identity (no transformation), Reinhard Color Transfer~\cite{reinhard2001color}, per-channel Linear Regression, and Histogram Matching.

\begin{table}[t]
\centering
\caption{Quantitative Comparison on Validation Set (1,619 images)}
\label{tab:quantitative}
\begin{tabular}{lccc}
\toprule
Method & PSNR (dB) $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
\midrule
\textbf{Pix2Pix} & \textbf{26.65} & \textbf{0.7454} & \textbf{0.2483} \\
NAFNet & 24.54 & 0.7386 & 0.3352 \\
\midrule
Channel Regression & 18.07 & 0.4244 & 0.5445 \\
Histogram Matching & 16.79 & 0.3210 & 0.6099 \\
Reinhard Color Transfer & 16.43 & 0.3998 & 0.5856 \\
Identity & 10.59 & 0.1962 & 0.5899 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:quantitative} shows that both deep learning methods substantially outperform traditional baselines, with pix2pix achieving the best results across all metrics. Compared to NAFNet, pix2pix provides a 2.1~dB improvement in PSNR and a significant 0.087 reduction in LPIPS (lower is better), while SSIM scores are comparable (0.745 vs 0.739). This suggests that adversarial training contributes meaningfully to perceptual quality for this task.

Both CNN methods dramatically outperform traditional approaches. Compared to the best traditional baseline (Channel Regression), pix2pix achieves an 8.6~dB improvement in PSNR, while even NAFNet provides a 6.5~dB improvement. The large gap between Identity and other methods confirms the substantial degradation introduced by the print-capture process. Traditional color transfer methods partially address color shifts but fail to restore fine details and texture, as evidenced by their high LPIPS scores.

\subsubsection{Qualitative Analysis}

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.32\textwidth}
\includegraphics[width=\textwidth]{fig_original.png}
\caption{Original}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\includegraphics[width=\textwidth]{fig_scanned.png}
\caption{Scanned}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\includegraphics[width=\textwidth]{fig_restored.png}
\caption{Restored}
\end{subfigure}
\caption{Image restoration results. (a) Original digital image. (b) Camera-captured image after printing, showing reduced contrast and color degradation. (c) Restored image produced by our model, recovering the original appearance.}
\label{fig:results}
\end{figure*}

Visual inspection of results demonstrates effective artifact removal across diverse image content (Fig.~\ref{fig:results}). The reverse model (captured $\rightarrow$ original) successfully corrects:

\begin{itemize}
\item \textbf{Color shifts}: Restores accurate color reproduction from captures exhibiting warm or cool bias
\item \textbf{Contrast reduction}: Recovers dynamic range compressed during printing
\item \textbf{Dark artifacts}: Removes vignetting and uneven illumination patterns
\item \textbf{Detail loss}: Sharpens edges softened by the print-capture process
\end{itemize}

The forward model (original $\rightarrow$ captured) learns to simulate print degradations, which serves as a useful augmentation tool and validates that the learned transformations are meaningful.

\subsubsection{Training Progression}

Results at steps 2K, 4K, 6K, 8K, and 10K show progressive improvement in restoration quality. Early checkpoints produce blurry outputs with color inaccuracies. By 6K steps, major artifacts are corrected but fine details remain soft. The final model at 10K steps achieves sharp reconstruction with accurate color and minimal visible artifacts.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig_baseline_comparison.png}
\caption{Visual comparison of restoration methods on sample validation images. From left to right: Input (camera capture), Reinhard Color Transfer, Channel Regression, Histogram Matching, Pix2Pix (Ours), and Ground Truth (original). Our method produces results most visually similar to the original images, recovering accurate colors and contrast.}
\label{fig:baseline_comparison}
\end{figure*}

\subsection{Ablation Studies}

\begin{table}[t]
\centering
\caption{Effect of Loss Components}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
Loss Configuration & Sharpness & Color & Artifacts \\
\midrule
L1 only & Medium & Good & Some \\
L1 + Perceptual & High & Good & Few \\
L1 + GAN & High & Medium & Few \\
Full (L1 + Perc + GAN) & High & Good & Minimal \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:ablation} summarizes qualitative effects of different loss configurations. L1 loss alone produces blurry results. Adding perceptual loss improves sharpness and detail preservation. The adversarial component further enhances high-frequency content and produces more natural-looking textures. The combination of all three losses yields the best overall results.

\subsection{Computational Efficiency}

Inference time for a single 512$\times$512 image is approximately 15ms on the RTX 4070 Ti, enabling real-time processing at over 60 frames per second. The model's 31M parameters result in a checkpoint size of approximately 125MB, suitable for deployment in resource-constrained environments.

Memory usage during training peaks at approximately 6-8GB with batch size 4, making the approach accessible on consumer-grade GPUs. The mixed-precision training reduces memory footprint by roughly 40\% compared to FP32.

\section{Conclusion}

We presented a comparative evaluation of two CNN architectures for restoring machine vision camera captures of printed images to their original digital quality. Our experiments demonstrate that pix2pix, combining a U-Net generator with PatchGAN discriminator and perceptual losses, outperforms NAFNet on this task despite NAFNet's strong performance on standard image restoration benchmarks. Pix2pix achieves 26.65~dB PSNR and 0.75 SSIM compared to NAFNet's 24.54~dB and 0.74, with the most notable difference in perceptual quality (LPIPS: 0.25 vs 0.34).

These results suggest that adversarial training provides meaningful benefits for print-capture restoration, likely because the task involves domain adaptation rather than simple noise removal. The discriminator helps the generator learn the specific characteristics of the target domain (original digital images) beyond what pixel-wise losses alone can capture.

Both deep learning approaches dramatically outperform traditional color transfer methods, confirming that learned representations are essential for modeling the complex, nonlinear degradations in the print-capture pipeline. The system trains efficiently on consumer hardware and processes images in real-time, making it practical for industrial deployment.

Future work includes investigating whether hybrid approaches combining NAFNet's efficient architecture with adversarial training could achieve the best of both worlds, and extending evaluation to different printing technologies and capture conditions.

\section*{Acknowledgment}

The authors thank the anonymous reviewers for their constructive feedback.

\begin{thebibliography}{00}
\bibitem{isola2017image} P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, ``Image-to-image translation with conditional adversarial networks,'' in \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2017, pp. 1125--1134. [Online]. Available: https://arxiv.org/abs/1611.07004

\bibitem{zhu2017unpaired} J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, ``Unpaired image-to-image translation using cycle-consistent adversarial networks,'' in \textit{Proc. IEEE Int. Conf. Comput. Vis. (ICCV)}, 2017, pp. 2223--2232. [Online]. Available: https://arxiv.org/abs/1703.10593

\bibitem{chen2018attention} X. Chen, C. Xu, X. Yang, and D. Tao, ``Attention-GAN for object transfiguration in wild images,'' in \textit{Proc. Eur. Conf. Comput. Vis. (ECCV)}, 2018, pp. 164--180. [Online]. Available: https://arxiv.org/abs/1803.06798

\bibitem{wang2018high} T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro, ``High-resolution image synthesis and semantic manipulation with conditional GANs,'' in \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2018, pp. 8798--8807. [Online]. Available: https://arxiv.org/abs/1711.11585

\bibitem{saharia2022palette} C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet, and M. Norouzi, ``Palette: Image-to-image diffusion models,'' in \textit{Proc. ACM SIGGRAPH}, 2022, pp. 1--10. [Online]. Available: https://arxiv.org/abs/2111.05826

\bibitem{hradis2015convolutional} M. Hradis, J. Kotera, P. Zemcik, and F. Sroubek, ``Convolutional neural networks for direct text deblurring,'' in \textit{Proc. Brit. Mach. Vis. Conf. (BMVC)}, 2015. [Online]. Available: https://www.fit.vutbr.cz/research/groups/graph/Deblurring/

\bibitem{tensmeyer2017document} C. Tensmeyer and T. Martinez, ``Document image binarization with fully convolutional neural networks,'' in \textit{Proc. Int. Conf. Document Anal. Recognit. (ICDAR)}, 2017, pp. 99--104. [Online]. Available: https://arxiv.org/abs/1708.03276

\bibitem{souibgui2020gan} M. A. Souibgui and Y. Kessentini, ``DE-GAN: A conditional generative adversarial network for document enhancement,'' \textit{IEEE Trans. Pattern Anal. Mach. Intell.}, vol. 44, no. 3, pp. 1180--1191, 2022. [Online]. Available: https://arxiv.org/abs/2010.02692

\bibitem{sharma2018understanding} A. Sharma, \textit{Understanding Color Management}, 2nd ed. Hoboken, NJ, USA: Wiley, 2018. [Online]. Available: https://www.wiley.com/en-us/Understanding+Color+Management

\bibitem{johnson2016perceptual} J. Johnson, A. Alahi, and L. Fei-Fei, ``Perceptual losses for real-time style transfer and super-resolution,'' in \textit{Proc. Eur. Conf. Comput. Vis. (ECCV)}, 2016, pp. 694--711. [Online]. Available: https://arxiv.org/abs/1603.08155

\bibitem{kaggle2013cats} Kaggle, ``Dogs vs.\ Cats,'' 2013. [Online]. Available: https://www.kaggle.com/c/dogs-vs-cats

\bibitem{wang2004ssim} Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ``Image quality assessment: From error visibility to structural similarity,'' \textit{IEEE Trans. Image Process.}, vol. 13, no. 4, pp. 600--612, Apr. 2004. [Online]. Available: https://ieeexplore.ieee.org/document/1284395

\bibitem{zhang2018lpips} R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ``The unreasonable effectiveness of deep features as a perceptual metric,'' in \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, 2018, pp. 586--595. [Online]. Available: https://arxiv.org/abs/1801.03924

\bibitem{reinhard2001color} E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley, ``Color transfer between images,'' \textit{IEEE Comput. Graph. Appl.}, vol. 21, no. 5, pp. 34--41, Sep./Oct. 2001. [Online]. Available: https://ieeexplore.ieee.org/document/946629

\bibitem{chen2022nafnet} L. Chen, X. Chu, X. Zhang, and J. Sun, ``Simple baselines for image restoration,'' in \textit{Proc. Eur. Conf. Comput. Vis. (ECCV)}, 2022, pp. 17--33. [Online]. Available: https://arxiv.org/abs/2204.04676
\end{thebibliography}

\end{document}
